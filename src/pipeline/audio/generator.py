"""
Google Cloud Text-to-Speech APIë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ìƒì„±ê¸°

SSMLì„ MP3ë¡œ ë³€í™˜í•˜ê³  ì •í™•í•œ íƒ€ì´ë° ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import os
import json
import tempfile
import subprocess
from typing import Dict, List, Any, Optional, Tuple
from google.cloud import texttospeech
from google.cloud.texttospeech_v1 import AudioConfig, AudioEncoding
import wave
import struct
import io

from .ssml_builder import SSMLBuilder


class AudioGenerator:
    """ì˜¤ë””ì˜¤ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, credentials_path: Optional[str] = None):
        """
        ì˜¤ë””ì˜¤ ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            credentials_path: Google Cloud ì¸ì¦ íŒŒì¼ ê²½ë¡œ
        """
        self.ssml_builder = SSMLBuilder()
        self.client = None
        self._initialize_client(credentials_path)
        
        self.audio_config = AudioConfig(
            audio_encoding=AudioEncoding.MP3,
            sample_rate_hertz=22050,
            effects_profile_id=["headphone-class-device"]
        )
    
    def _initialize_client(self, credentials_path: Optional[str] = None):
        """Google Cloud TTS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            if credentials_path and os.path.exists(credentials_path):
                os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials_path
            self.client = texttospeech.TextToSpeechClient()
            print("âœ… Google Cloud TTS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ Google Cloud TTS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.client = None
    
    def _get_emotion_settings(self, script_type: str) -> Dict[str, Any]:
        """ìŠ¤í¬ë¦½íŠ¸ íƒ€ì…ë³„ ê°ì • ì„¤ì • ë°˜í™˜"""
        
        if script_type in ["ì¸íŠ¸ë¡œ", "intro"]:
            return {
                "use_ssml": False,  # SSML ë¯¸ì§€ì› í™”ì ë•Œë¬¸ì— Falseë¡œ ì„¤ì •
                "rate": "1.0",
                "pitch": "+1st", 
                "volume": "0dB",
                "emphasis": "moderate",
                "effects_profile": ["headphone-class-device", "telephony-class-application"],
                "description": "ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ ëŒ€í™” (ì¼ë°˜ í…ìŠ¤íŠ¸)"
            }
        
        elif script_type in ["íšŒí™”", "ëŒ€í™”", "conversation"]:
            return {
                "use_ssml": False,  # SSML ë¯¸ì§€ì› í™”ì ë•Œë¬¸ì— Falseë¡œ ì„¤ì •
                "rate": "1.0",  # ì •ìƒ ì†ë„ë¡œ ì •í™•í•œ ë°œìŒ
                "pitch": "0st",  # ì¤‘ë¦½ì  í†¤
                "volume": "0dB",  # ì •ìƒ ë³¼ë¥¨
                "emphasis": "none",  # ê°•ì¡° ì—†ìŒ (êµìœ¡ìš©)
                "effects_profile": ["headphone-class-device"],  # ê¸°ë³¸ í”„ë¡œí•„ë§Œ
                "description": "êµìœ¡ìš© ì •í™•í•œ ë°œìŒ (ê°ì • ìµœì†Œí™”)"
            }
        
        elif script_type in ["ì—”ë”©", "ending"]:
            return {
                "use_ssml": False,  # SSML ë¯¸ì§€ì› í™”ì ë•Œë¬¸ì— Falseë¡œ ì„¤ì •
                "rate": "0.9",
                "pitch": "0st", 
                "volume": "0dB",
                "emphasis": "moderate",
                "effects_profile": ["headphone-class-device", "telephony-class-application"],
                "description": "ë”°ëœ»í•˜ê³  ë§ˆë¬´ë¦¬í•˜ëŠ” í†¤ (ì¼ë°˜ í…ìŠ¤íŠ¸)"
            }
        
        else:  # ê¸°ë³¸ê°’
            return {
                "use_ssml": False,
                "effects_profile": ["headphone-class-device"],
                "description": "ê¸°ë³¸ ì„¤ì •"
            }

    def _add_comma_pauses(self, text: str, script_type: str) -> str:
        """ì‰¼í‘œì™€ ìŠ¬ë˜ì‹œ ìœ„ì¹˜ì— ìì—°ìŠ¤ëŸ¬ìš´ íœ´ì§€ ì¶”ê°€ (SSMLìš©)"""
        pause_time = self._get_comma_pause_time(script_type)
        # ì‰¼í‘œì™€ ìŠ¬ë˜ì‹œ ëª¨ë‘ íœ´ì§€ë¡œ ëŒ€ì²´
        text_with_pauses = text.replace(',', f'<break time="{pause_time}"/>').replace('/', f'<break time="{pause_time}"/>')
        return text_with_pauses
    
    def _add_comma_pauses_plain_text(self, text: str, script_type: str) -> str:
        """ì‰¼í‘œì™€ ìŠ¬ë˜ì‹œ ìœ„ì¹˜ì— ìì—°ìŠ¤ëŸ¬ìš´ íœ´ì§€ ì¶”ê°€ (ì¼ë°˜ í…ìŠ¤íŠ¸ìš©)"""
        pause_time = self._get_comma_pause_time(script_type)
        # ì‰¼í‘œì™€ ìŠ¬ë˜ì‹œë¥¼ ì‹¤ì œ íœ´ì§€ ì‹œê°„ìœ¼ë¡œ ëŒ€ì²´
        if pause_time == "0.5s":
            pause_dots = "..."  # ì¸íŠ¸ë¡œ/íšŒí™”ìš©
        elif pause_time == "0.6s":
            pause_dots = "...."  # ì—”ë”©ìš©
        elif pause_time == "0.2s":
            pause_dots = "."  # ìµœì†Œ íœ´ì§€
        else:
            pause_dots = ".."  # ê¸°ë³¸ê°’
        
        # ì‰¼í‘œì™€ ìŠ¬ë˜ì‹œ ëª¨ë‘ íœ´ì§€ë¡œ ëŒ€ì²´
        text_with_pauses = text.replace(',', pause_dots).replace('/', pause_dots)
        return text_with_pauses

    def _get_comma_pause_time(self, script_type: str) -> str:
        """ìŠ¤í¬ë¦½íŠ¸ íƒ€ì…ë³„ ì‰¼í‘œ íœ´ì§€ ì‹œê°„ ë°˜í™˜"""
        
        if script_type in ["ì¸íŠ¸ë¡œ", "intro"]:
            return "0.5s"  # ìì—°ìŠ¤ëŸ¬ìš´ ì¸ì‚¬ë§
        elif script_type in ["ì—”ë”©", "ending"]:
            return "0.6s"  # ë§ˆë¬´ë¦¬ ì¸ì‚¬ (ì¡°ê¸ˆ ë” ëŠê¸‹í•˜ê²Œ)
        elif script_type in ["íšŒí™”", "ëŒ€í™”", "conversation"]:
            return "0.5s"  # íšŒí™”ì—ì„œë„ 0.5ì´ˆ íœ´ì§€
        else:
            return "0.3s"  # ê¸°ë³¸ê°’

    def _create_emotional_ssml(self, text: str, emotion_settings: Dict[str, Any], 
                              script_type: str = "conversation") -> str:
        """ê°ì • ì„¤ì •ì— ë”°ë¥¸ SSML ìƒì„± (ì‰¼í‘œ íœ´ì§€ í¬í•¨, í™”ì ì œì™¸)"""
        
        # ì¸íŠ¸ë¡œ/ì—”ë”©ì¸ ê²½ìš° ì‰¼í‘œì— íœ´ì§€ ì¶”ê°€
        if script_type in ["ì¸íŠ¸ë¡œ", "ì—”ë”©", "intro", "ending"]:
            text = self._add_comma_pauses(text, script_type)
        
        # SSMLì—ëŠ” ê°ì • í‘œí˜„ë§Œ í¬í•¨ (í™”ìëŠ” ë³„ë„ë¡œ ì§€ì •)
        return f"""<speak>
<prosody rate="{emotion_settings['rate']}" 
         pitch="{emotion_settings['pitch']}" 
         volume="{emotion_settings['volume']}">
    <emphasis level="{emotion_settings['emphasis']}">
        {text}
    </emphasis>
</prosody>
</speak>"""

    def _synthesize_segment_audio(self, text: str, output_path: str, voice_name: str, 
                                 language_code: str, script_type: str = "conversation") -> float:
        """ê°œë³„ ì„¸ê·¸ë¨¼íŠ¸ ì˜¤ë””ì˜¤ ìƒì„± ë° ì •í™•í•œ ê¸¸ì´ ë°˜í™˜ (ê°ì • í‘œí˜„ ë° íœ´ì§€ í¬í•¨)"""
        try:
            if not self.client:
                print("âŒ Google TTS í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return 0.0
            
            # ê°ì • ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            emotion_settings = self._get_emotion_settings(script_type)
            
            # í™”ìëŠ” Google TTS API í˜¸ì¶œ ì‹œì— ë³„ë„ë¡œ ì§€ì •
            voice = texttospeech.VoiceSelectionParams(language_code=language_code, name=voice_name)
            
            # ê°ì • ì„¤ì •ì— ë”°ë¥¸ AudioConfig ìƒì„±
            audio_config = AudioConfig(
                audio_encoding=AudioEncoding.MP3,
                sample_rate_hertz=22050,
                effects_profile_id=emotion_settings.get("effects_profile", ["headphone-class-device"])
            )
            
            # SSML ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì…ë ¥ ë°©ì‹ ê²°ì •
            if emotion_settings.get("use_ssml", False):
                # SSMLë¡œ ê°ì • í‘œí˜„ ë° íœ´ì§€ í¬í•¨ (í™”ìëŠ” ë³„ë„ ì§€ì •)
                ssml_text = self._create_emotional_ssml(text, emotion_settings, script_type)
                synthesis_input = texttospeech.SynthesisInput(ssml=ssml_text)
                print(f"ğŸµ ê°ì • í‘œí˜„ ì ìš©: {emotion_settings['description']}")
                
                # íœ´ì§€ ì‹œê°„ ë¡œê¹…
                if script_type in ["ì¸íŠ¸ë¡œ", "ì—”ë”©", "íšŒí™”", "ëŒ€í™”", "intro", "ending", "conversation"]:
                    comma_count = text.count(',')
                    slash_count = text.count('/')
                    total_pause_count = comma_count + slash_count
                    pause_time = self._get_comma_pause_time(script_type)
                    pause_seconds = float(pause_time.replace('s', ''))
                    total_pause_time = total_pause_count * pause_seconds
                    
                    if total_pause_count > 0:
                        print(f"ğŸµ íœ´ì§€ ì •ë³´: ì‰¼í‘œ {comma_count}ê°œ, ìŠ¬ë˜ì‹œ {slash_count}ê°œ, íœ´ì§€ {pause_seconds}ì´ˆì”©, ì´ {total_pause_time}ì´ˆ")
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ (ì‰¼í‘œ íœ´ì§€ í¬í•¨)
                text_with_pauses = self._add_comma_pauses_plain_text(text, script_type)
                synthesis_input = texttospeech.SynthesisInput(text=text_with_pauses)
                
                # íœ´ì§€ ì‹œê°„ ë¡œê¹…
                if script_type in ["ì¸íŠ¸ë¡œ", "ì—”ë”©", "íšŒí™”", "ëŒ€í™”", "intro", "ending", "conversation"]:
                    comma_count = text.count(',')
                    slash_count = text.count('/')
                    total_pause_count = comma_count + slash_count
                    pause_time = self._get_comma_pause_time(script_type)
                    pause_seconds = float(pause_time.replace('s', ''))
                    total_pause_time = total_pause_count * pause_seconds
                    
                    if total_pause_count > 0:
                        print(f"ğŸµ íœ´ì§€ ì •ë³´: ì‰¼í‘œ {comma_count}ê°œ, ìŠ¬ë˜ì‹œ {slash_count}ê°œ, íœ´ì§€ {pause_seconds}ì´ˆì”©, ì´ {total_pause_time}ì´ˆ")
            
            # Google TTS API í˜¸ì¶œ (í™”ìëŠ” voice íŒŒë¼ë¯¸í„°ë¡œ ë³„ë„ ì§€ì •)
            response = self.client.synthesize_speech(
                input=synthesis_input,
                voice=voice,
                audio_config=audio_config
            )
            
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, "wb") as out:
                out.write(response.audio_content)
            
            duration = self._get_accurate_audio_duration(output_path)
            print(f"âœ… ì„¸ê·¸ë¨¼íŠ¸ ì˜¤ë””ì˜¤ ìƒì„±: {os.path.basename(output_path)} ({duration:.2f}ì´ˆ)")
            return duration
        except Exception as e:
            print(f"âŒ ì„¸ê·¸ë¨¼íŠ¸ ì˜¤ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return 0.0

    def _create_silence_segment(self, duration_seconds: int, output_path: str) -> Optional[str]:
        """ì§€ì •ëœ ê¸¸ì´ì˜ ë¬´ìŒ MP3 íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            command = f'ffmpeg -f lavfi -i anullsrc=r=22050:cl=mono -t {duration_seconds} -q:a 9 -acodec libmp3lame "{output_path}"'
            subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
            print(f"âœ… ë¬´ìŒ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±: {os.path.basename(output_path)}")
            return output_path
        except Exception as e:
            print(f"âŒ ë¬´ìŒ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def merge_audio_segments(self, segment_paths: List[str], output_path: str) -> bool:
        """FFmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©í•©ë‹ˆë‹¤."""
        if not segment_paths:
            print("ë³‘í•©í•  ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        try:
            list_content = ""
            for path in segment_paths:
                safe_path = os.path.abspath(path).replace('\\', '/')
                list_content += f"file '{safe_path}'\n"
            
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=".txt", encoding='utf-8') as tmp_list_file:
                tmp_list_file.write(list_content)
                merge_list_path = tmp_list_file.name

            command = f'ffmpeg -f concat -safe 0 -i "{merge_list_path}" -c copy "{output_path}" -y'
            subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
            
            print(f"âœ… ì˜¤ë””ì˜¤ ë³‘í•© ì™„ë£Œ: {output_path}")
            os.remove(merge_list_path)
            return True
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ë³‘í•© ì‹¤íŒ¨: {e}")
            if 'merge_list_path' in locals() and os.path.exists(merge_list_path):
                os.remove(merge_list_path)
            return False
    
    def _load_speaker_config(self, output_dir: str, identifier: str) -> Dict[str, Any]:
        """í™”ì ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # output_dirëŠ” mp3 ë””ë ‰í† ë¦¬ì´ë¯€ë¡œ, ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ í™”ì ì„¤ì • íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤
            # output_dir: output/kor-chn/kor-chn/mp3
            # base_dir: output/kor-chn/kor-chn
            base_dir = os.path.dirname(output_dir)  # mp3 ë””ë ‰í† ë¦¬ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬
            speaker_config_path = os.path.join(base_dir, f"{identifier}_speaker.json")
            
            print(f"ğŸ” í™”ì ì„¤ì • íŒŒì¼ ê²½ë¡œ í™•ì¸: {speaker_config_path}")
            
            if not os.path.exists(speaker_config_path):
                print(f"âš ï¸ í™”ì ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {speaker_config_path}")
                # ëŒ€ì•ˆ ê²½ë¡œë“¤ë„ í™•ì¸í•´ë³´ê¸°
                alternative_paths = [
                    os.path.join(output_dir, f"{identifier}_speaker.json"),  # mp3 ë””ë ‰í† ë¦¬ ë‚´
                    os.path.join(os.path.dirname(base_dir), f"{identifier}_speaker.json"),  # ë” ìƒìœ„ ë””ë ‰í† ë¦¬
                ]
                
                for alt_path in alternative_paths:
                    if os.path.exists(alt_path):
                        print(f"âœ… ëŒ€ì•ˆ ê²½ë¡œì—ì„œ í™”ì ì„¤ì • íŒŒì¼ ë°œê²¬: {alt_path}")
                        speaker_config_path = alt_path
                        break
                else:
                    print(f"âŒ ëª¨ë“  ê²½ë¡œì—ì„œ í™”ì ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return {}
            
            with open(speaker_config_path, 'r', encoding='utf-8') as f:
                speaker_config = json.load(f)
            
            print(f"âœ… í™”ì ì„¤ì • ë¡œë“œ ì™„ë£Œ: {speaker_config_path}")
            print(f"  - ì›ì–´ í™”ì: {speaker_config.get('native_speaker', 'N/A')}")
            print(f"  - í•™ìŠµì–´ í™”ì ìˆ˜: {speaker_config.get('learner_count', 'N/A')}")
            print(f"  - í•™ìŠµì–´ í™”ì ëª©ë¡: {speaker_config.get('learner_speakers', [])}")
            
            return speaker_config
        except Exception as e:
            print(f"âŒ í™”ì ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}

    def generate_audio_and_timing(self, manifest_data: Dict, output_dir: str, script_type: str) -> Tuple[Optional[str], Optional[Dict]]:
        """
        (Refactored) ì˜¤ë””ì˜¤ì™€ íƒ€ì´ë° ì •ë³´ë¥¼ ì¼ê´€ëœ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            print("ğŸµ [í†µí•©] ì˜¤ë””ì˜¤ ë° íƒ€ì´ë° ìƒì„± ì‹œì‘...")
            timing_info = {"total_duration": 0.0, "segments": []}
            segment_paths_for_merge = []
            temp_dir = tempfile.mkdtemp(prefix="audio_temp_")
            current_time = 0.0
            
            project_name = manifest_data.get("project_name", "untitled_project")
            identifier = manifest_data.get("identifier", project_name)
            
            # í™”ì ì„¤ì • ë¡œë“œ
            speaker_config = self._load_speaker_config(output_dir, identifier)
            native_speaker = speaker_config.get('native_speaker', 'ko-KR-Standard-A')
            learner_speakers = speaker_config.get('learner_speakers', ['cmn-CN-Standard-A', 'cmn-CN-Standard-B', 'cmn-CN-Standard-C', 'cmn-CN-Standard-D'])

            print("\n" + "="*50)
            print("ğŸµ ì˜¤ë””ì˜¤ íƒ€ì´ë° ê³„ì‚° ë¡œê·¸")
            print("="*50)
            
            # ìŠ¤í¬ë¦½íŠ¸ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
            if script_type in ["íšŒí™”", "ëŒ€í™”"]:
                # íšŒí™”/ëŒ€í™” íƒ€ì… ì²˜ë¦¬
                for scene in manifest_data.get('scenes', []):
                    if scene.get('type') == 'conversation':
                        sequence = scene.get('sequence', 1)
                        
                        native_script = scene.get('native_script', '')
                        if native_script:
                            screen1_audio_path = os.path.join(temp_dir, f"segment_{sequence:03d}_screen1.mp3")
                            screen1_duration = self._synthesize_segment_audio(
                                native_script, screen1_audio_path, 
                                voice_name=native_speaker, language_code="ko-KR", script_type=script_type
                            )
                            if screen1_duration > 0:
                                segment_paths_for_merge.append(screen1_audio_path)
                                silence_path = self._create_silence_segment(1, os.path.join(temp_dir, f"silence_{sequence:03d}_s1.mp3"))
                                if not silence_path: raise Exception("Silence creation failed")
                                segment_paths_for_merge.append(silence_path)

                                screen1_end_time = current_time + screen1_duration + 1.0
                                print(f"  - ì¥ë©´ {sequence} (Screen 1):")
                                print(f"    - ì›ì–´ í™”ì: {native_speaker}")
                                print(f"    - ì‹œì‘: {current_time:.3f}ì´ˆ")
                                print(f"    - ìŒì„± ê¸¸ì´: {screen1_duration:.3f}ì´ˆ")
                                print(f"    - ë¬´ìŒ í¬í•¨ ì¢…ë£Œ: {screen1_end_time:.3f}ì´ˆ")
                                print("  "+"-"*20)
                                
                                timing_info["segments"].append({
                                    "name": f"{identifier}_{sequence:03d}_screen1.png",
                                    "start_time": round(current_time, 2),
                                    "end_time": round(screen1_end_time, 2),
                                    "duration": round(screen1_duration + 1.0, 2),
                                    "text": native_script
                                })
                                current_time = screen1_end_time
                        
                        learning_script = scene.get('learning_script', '')
                        reading_script = scene.get('reading_script', '')
                        if learning_script or reading_script:
                            screen2_start_time = current_time
                            print(f"  - ì¥ë©´ {sequence} (Screen 2):")
                            print(f"    - Screen 2 ì‹œì‘: {screen2_start_time:.3f}ì´ˆ")
                            
                            learner_texts = [learning_script, reading_script, learning_script, reading_script]
                            screen2_total_duration = 0.0
                            
                            for i in range(1, 5):
                                text_to_speak = learner_texts[i-1].strip()
                                if text_to_speak:
                                    learner_audio_path = os.path.join(temp_dir, f"segment_{sequence:03d}_screen2_learner_{i}.mp3")
                                    # í•™ìŠµì–´ í™”ì ìˆœì„œëŒ€ë¡œ ì‚¬ìš© (1-4ë²ˆ í™”ì)
                                    learner_voice = learner_speakers[i-1] if i-1 < len(learner_speakers) else learner_speakers[0]
                                    learner_duration = self._synthesize_segment_audio(
                                        text_to_speak, learner_audio_path,
                                        voice_name=learner_voice, language_code="cmn-CN", script_type=script_type
                                    )
                                    if learner_duration > 0:
                                        segment_paths_for_merge.append(learner_audio_path)
                                        silence_path = self._create_silence_segment(1, os.path.join(temp_dir, f"silence_{sequence:03d}_s2_l{i}.mp3"))
                                        if not silence_path: raise Exception("Silence creation failed")
                                        segment_paths_for_merge.append(silence_path)
                                        print(f"      - í•™ìŠµì {i} ìŒì„± ({learner_voice}): {learner_duration:.3f}ì´ˆ / ë¬´ìŒ í¬í•¨: {learner_duration + 1.0:.3f}ì´ˆ")
                                        screen2_total_duration += learner_duration + 1.0
                            
                            if screen2_total_duration > 0:
                                screen2_end_time = screen2_start_time + screen2_total_duration
                                print(f"    - Screen 2 ì´í•© (ë¬´ìŒ í¬í•¨): {screen2_total_duration:.3f}ì´ˆ")
                                print(f"    - Screen 2 ì¢…ë£Œ: {screen2_end_time:.3f}ì´ˆ")
                                print("  "+"-"*20)

                                screen2_text = f"{learning_script} {reading_script}".strip()
                                timing_info["segments"].append({
                                    "name": f"{identifier}_{sequence:03d}_screen2.png",
                                    "start_time": round(screen2_start_time, 2),
                                    "end_time": round(screen2_end_time, 2),
                                    "duration": round(screen2_total_duration, 2),
                                    "text": screen2_text
                                })
                                current_time = screen2_end_time
            
            elif script_type in ["ì¸íŠ¸ë¡œ", "ì—”ë”©"]:
                # ì¸íŠ¸ë¡œ/ì—”ë”© íƒ€ì… ì²˜ë¦¬ - ë¼ì¸ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
                for scene in manifest_data.get('scenes', []):
                    if scene.get('type') in ['intro', 'ending']:
                        sequence = scene.get('sequence', 1)
                        script_text = scene.get('script', '') or scene.get('full_script', '')
                        
                        if script_text:
                            # ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¼ì¸ë³„ë¡œ ë¶„ë¦¬
                            lines = [line.strip() for line in script_text.split('\n') if line.strip()]
                            
                            print(f"  - ì¥ë©´ {sequence} ({script_type}):")
                            print(f"    - ì´ {len(lines)}ê°œ ë¼ì¸ ì²˜ë¦¬")
                            print(f"    - í™”ì: {native_speaker}")
                            print(f"    - ì‹œì‘: {current_time:.3f}ì´ˆ")
                            
                            scene_start_time = current_time
                            
                            for line_idx, line in enumerate(lines):
                                if line:  # ë¹ˆ ë¼ì¸ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì²˜ë¦¬
                                    audio_path = os.path.join(temp_dir, f"segment_{sequence:03d}_line_{line_idx+1:02d}.mp3")
                                    duration = self._synthesize_segment_audio(
                                        line, audio_path, 
                                        voice_name=native_speaker, language_code="ko-KR", script_type=script_type
                                    )
                                    
                                    if duration > 0:
                                        segment_paths_for_merge.append(audio_path)
                                        
                                        # ë§ˆì§€ë§‰ ë¼ì¸ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë¬´ìŒ ì¶”ê°€
                                        if line_idx < len(lines) - 1:
                                            silence_path = self._create_silence_segment(1, os.path.join(temp_dir, f"silence_{sequence:03d}_line_{line_idx+1:02d}.mp3"))
                                            if not silence_path: raise Exception("Silence creation failed")
                                            segment_paths_for_merge.append(silence_path)
                                        
                                        end_time = current_time + duration + (1.0 if line_idx < len(lines) - 1 else 0.0)
                                        
                                        print(f"      - ë¼ì¸ {line_idx+1}: {line[:30]}{'...' if len(line) > 30 else ''}")
                                        print(f"        - ìŒì„± ê¸¸ì´: {duration:.3f}ì´ˆ")
                                        print(f"        - ì¢…ë£Œ: {end_time:.3f}ì´ˆ")
                                        
                                        # ê° ë¼ì¸ë³„ íƒ€ì´ë° ì •ë³´ ì¶”ê°€
                                        # ìŠ¤í¬ë¦½íŠ¸ íƒ€ì…ì„ ì˜ì–´ë¡œ ë³€í™˜
                                        english_script_type = {"ì¸íŠ¸ë¡œ": "intro", "ì—”ë”©": "ending"}.get(script_type, script_type.lower())
                                        timing_info["segments"].append({
                                            "name": f"{identifier}_{english_script_type}_{line_idx+1:03d}.png",
                                            "start_time": round(current_time, 2),
                                            "end_time": round(end_time, 2),
                                            "duration": round(duration + (1.0 if line_idx < len(lines) - 1 else 0.0), 2),
                                            "text": line
                                        })
                                        
                                        current_time = end_time
                            
                            scene_end_time = current_time
                            print(f"    - ì¥ë©´ ì´ ê¸¸ì´: {scene_end_time - scene_start_time:.3f}ì´ˆ")
                            print("  "+"-"*20)
            
            # ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if not segment_paths_for_merge:
                print(f"âš ï¸ {script_type} ìŠ¤í¬ë¦½íŠ¸ì— ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                # ë¹ˆ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± (1ì´ˆ ë¬´ìŒ)
                empty_audio_path = os.path.join(temp_dir, "empty.mp3")
                silence_path = self._create_silence_segment(1, empty_audio_path)
                if silence_path:
                    segment_paths_for_merge.append(silence_path)
                    timing_info["segments"].append({
                        "name": f"{identifier}_001.png",
                        "start_time": 0.0,
                        "end_time": 1.0,
                        "duration": 1.0,
                        "text": ""
                    })
                    current_time = 1.0
            
            print("="*50 + "\n")

            english_script_type = {"íšŒí™”": "conversation", "ëŒ€í™”": "conversation", "ì¸íŠ¸ë¡œ": "intro", "ì—”ë”©": "ending"}.get(script_type, script_type)
            final_audio_path = os.path.join(output_dir, f"{identifier}_{english_script_type}.mp3")
            
            print(f"ğŸµ ìƒì„±ëœ ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•© ì¤‘... -> {final_audio_path}")
            merge_success = self.merge_audio_segments(segment_paths_for_merge, final_audio_path)
            if not merge_success:
                raise Exception("Audio segment merging failed.")
            
            print(f"ğŸ” ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ ê²€ì¦: {final_audio_path}")
            actual_duration = self._get_accurate_audio_duration(final_audio_path)
            if actual_duration > 0:
                print(f"ğŸµ ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´: {actual_duration:.3f}ì´ˆ (ì´ ê°’ì„ ìµœì¢… ì‚¬ìš©)")
                timing_info["total_duration"] = round(actual_duration, 2)
                if timing_info["segments"]:
                    last_segment = timing_info["segments"][-1]
                    last_segment["end_time"] = round(actual_duration, 2)
                    last_segment["duration"] = round(actual_duration - last_segment["start_time"], 2)
                    print(f"ğŸ”§ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì´ë° ë³´ì •: {last_segment['start_time']:.2f}ì´ˆ ~ {last_segment['end_time']:.2f}ì´ˆ")
            else:
                print(f"âš ï¸ ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ë¥¼ ì¸¡ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì‚°ëœ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {current_time:.2f}ì´ˆ")
                timing_info["total_duration"] = round(current_time, 2)

            timing_info["final_audio_path"] = final_audio_path
            
            for p in segment_paths_for_merge:
                try:
                    os.remove(p)
                except OSError:
                    pass
            try:
                os.rmdir(temp_dir)
            except OSError:
                pass
            
            print(f"âœ… [í†µí•©] ì˜¤ë””ì˜¤ ë° íƒ€ì´ë° ìƒì„± ì™„ë£Œ: {len(timing_info['segments'])}ê°œ ì„¸ê·¸ë¨¼íŠ¸, ì´ {timing_info['total_duration']:.2f}ì´ˆ")
            return final_audio_path, timing_info
            
        except Exception as e:
            import traceback
            print(f"âŒ [í†µí•©] ì˜¤ë””ì˜¤ ë° íƒ€ì´ë° ìƒì„± ì‹¤íŒ¨: {e}")
            print(traceback.format_exc())
            return None, None

    def save_precise_timing_info(self, timing_info: Dict, output_path: str) -> bool:
        """ì •í™•í•œ íƒ€ì´ë° ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(timing_info, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ì •í™•í•œ íƒ€ì´ë° ì •ë³´ ì €ì¥: {output_path}")
            print(f"   - ì´ {len(timing_info.get('segments', []))}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            print(f"   - ì´ ì¬ìƒì‹œê°„: {timing_info.get('total_duration', 0):.2f}ì´ˆ")
            
            return True
            
        except Exception as e:
            print(f"âŒ íƒ€ì´ë° ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
        
    def _get_accurate_audio_duration(self, audio_path: str) -> float:
        """FFmpegë¡œ ì •í™•í•œ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •"""
        try:
            import subprocess
            import json
            
            if not os.path.exists(audio_path):
                print(f"âš ï¸ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {audio_path}")
                return 0.0
            
            cmd = [
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_format', audio_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                print(f"âš ï¸ FFprobe ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
                return self._fallback_duration_estimation(audio_path)
            
            audio_info = json.loads(result.stdout)
            duration = float(audio_info['format']['duration'])
            
            print(f"ğŸµ ì •í™•í•œ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •: {audio_path} ({duration:.2f}ì´ˆ)")
            return round(duration, 2)
            
        except Exception as e:
            print(f"âš ï¸ FFmpeg ê¸°ë°˜ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨: {e}")
            return self._fallback_duration_estimation(audio_path)
    
    def _fallback_duration_estimation(self, audio_path: str) -> float:
        """FFmpeg ì‹¤íŒ¨ ì‹œ íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¶”ì •"""
        try:
            file_size = os.path.getsize(audio_path)
            bitrate = 128 * 1000  # bits per second
            duration = (file_size * 8) / bitrate
            print(f"ğŸ“Š íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¶”ì •: {audio_path} ({duration:.2f}ì´ˆ)")
            return round(duration, 2)
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì™„ì „ ì‹¤íŒ¨: {e}")
            return 0.0
