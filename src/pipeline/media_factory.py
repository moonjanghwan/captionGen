# src/pipeline/media_factory.py

import os
import json
import subprocess
import tempfile
from typing import Dict, Any, List, Tuple
from google.cloud import texttospeech

from ..settings import MergedSettings
from .renderers.png_renderer import PNGRenderer

class MediaFactory:
    """
    ì˜¤ë””ì˜¤, ì´ë¯¸ì§€, íƒ€ì„ë¼ì¸, ë¹„ë””ì˜¤ ìƒì„±ì„ ì´ê´„í•˜ëŠ” í†µí•© í´ë˜ìŠ¤.
    ì œì‘ ì‚¬ì–‘ì„œì— ë”°ë¥¸ ë³µì¡í•œ ë¯¸ë””ì–´ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    def __init__(self, merged_settings: MergedSettings):
        self.settings = merged_settings
        self.png_renderer = PNGRenderer(merged_settings)
        self.tts_client = texttospeech.TextToSpeechClient()
        print("âœ… MediaFactory ì´ˆê¸°í™” ì™„ë£Œ")

    # --- SSML ë¹Œë” ë¡œì§ ---
    def _build_conversation_ssml(self, scene_data: Dict[str, Any]) -> str:
        # ... (íšŒí™”ìš© SSML ìƒì„± ë¡œì§) ...
        return f"""<?xml version="1.0"?>
<speak version="1.1" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="ko-KR">
    <voice name="{scene_data['native_voice']}"><mark name="scene_{scene_data['sequence']}_screen1_start"/><prosody rate="medium">{scene_data['native_script']}</prosody><mark name="scene_{scene_data['sequence']}_screen1_end"/></voice>
    <break time="1s"/>
    <mark name="scene_{scene_data['sequence']}_screen2_start"/>
    <voice name="{scene_data['learner_voices'][0]}"><prosody rate="medium">{scene_data['learning_script']}</prosody></voice><break time="1s"/>
    <voice name="{scene_data['learner_voices'][1]}"><prosody rate="medium">{scene_data['learning_script']}</prosody></voice><break time="1s"/>
    <voice name="{scene_data['learner_voices'][2]}"><prosody rate="medium">{scene_data['learning_script']}</prosody></voice><break time="1s"/>
    <voice name="{scene_data['learner_voices'][3]}"><prosody rate="medium">{scene_data['learning_script']}</prosody></voice>
    <mark name="scene_{scene_data['sequence']}_screen2_end"/>
</speak>"""

    def _build_intro_ending_ssml(self, full_script: str, voice_name: str) -> str:
        # ... (ì¸íŠ¸ë¡œ/ì—”ë”©ìš© SSML ìƒì„± ë¡œì§) ...
        sentences = [s.strip() for s in full_script.split('\n') if s.strip()]
        ssml_body = ""
        for i, sentence in enumerate(sentences):
            ssml_body += f'<mark name="sentence_{i+1}_start"/><prosody rate="medium">{sentence}</prosody><mark name="sentence_{i+1}_end"/><break time="1s"/>'
        return f"""<?xml version="1.0"?><speak version="1.1" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="ko-KR"><voice name="{voice_name}">{ssml_body}</voice></speak>"""


    # --- ì˜¤ë””ì˜¤ ë° íƒ€ì´ë° ìƒì„± ---
    def create_audio_and_timing(self, ssml_content: str, output_audio_path: str, output_timing_path: str) -> bool:
        print(f"ğŸ”Š [ì˜¤ë””ì˜¤ ìƒì„±] ì‹œì‘: {os.path.basename(output_audio_path)}")
        try:
            synthesis_input = texttospeech.SynthesisInput(ssml=ssml_content)
            voice = texttospeech.VoiceSelectionParams(language_code="ko-KR") # ê¸°ë³¸ê°’, SSML ë‚´ë¶€ voice íƒœê·¸ê°€ ìš°ì„ 
            audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3, sample_rate_hertz=22050)
            
            request = texttospeech.SynthesizeSpeechRequest(
                input=synthesis_input,
                voice=voice,
                audio_config=audio_config,
                enable_time_pointing=[texttospeech.SynthesizeSpeechRequest.TimepointType.SSML_MARK]
            )
            response = self.tts_client.synthesize_speech(request=request)

            with open(output_audio_path, "wb") as out:
                out.write(response.audio_content)
            print(f"âœ… [ì˜¤ë””ì˜¤ ìƒì„±] ì™„ë£Œ: {os.path.basename(output_audio_path)}")

            timing_data = [{"mark_name": timepoint.mark_name, "time_seconds": timepoint.time_seconds} for timepoint in response.timepoints]
            with open(output_timing_path, "w", encoding="utf-8") as f:
                json.dump(timing_data, f, indent=2)
            print(f"âœ… [íƒ€ì´ë° ì¶”ì¶œ] ì™„ë£Œ: {os.path.basename(output_timing_path)}")
            return True
        except Exception as e:
            print(f"âŒ [ì˜¤ë””ì˜¤/íƒ€ì´ë° ìƒì„±] ì‹¤íŒ¨: {e}")
            return False

    # --- íƒ€ì„ë¼ì¸ ë¹Œë” ---
    def build_timeline(self, manifest_data: Dict[str, Any], timing_data: List[Dict[str, Any]], image_dir: str, audio_path: str) -> Dict[str, Any]:
        # ... (ì´ì „ ë‹µë³€ì˜ íƒ€ì„ë¼ì¸ ë¹Œë” ë¡œì§ê³¼ ìœ ì‚¬) ...
        pass

    # --- ë¹„ë””ì˜¤ ë Œë”ëŸ¬ ---
    def render_video(self, timeline_data: Dict[str, Any], output_video_path: str) -> bool:
        # ... (ì´ì „ ë‹µë³€ì˜ ë¹„ë””ì˜¤ ìƒì„± ë¡œì§ê³¼ ìœ ì‚¬) ...
        pass

    # --- í†µí•© íŒŒì´í”„ë¼ì¸ ---
    def create_conversation_video(self, manifest_scene: Dict[str, Any], paths: Dict[str, str], resolution: Tuple[int, int]) -> bool:
        """íšŒí™” ë¹„ë””ì˜¤ ì œì‘ íŒŒì´í”„ë¼ì¸"""
        # 1. SSML ìƒì„± (ì‚¬ì–‘ì„œì— ë§ê²Œ)
        # ... 
        
        # 2. ì˜¤ë””ì˜¤ ë° íƒ€ì´ë° ìƒì„±
        # ...

        # 3. ì´ë¯¸ì§€ ìƒì„± (2ê°œ í™”ë©´)
        # ...
        
        # 4. íƒ€ì„ë¼ì¸ ìƒì„±
        # ...

        # 5. ë¹„ë””ì˜¤ ë Œë”ë§
        # ...
        return True # ì„±ê³µ ì—¬ë¶€ ë°˜í™˜

    def create_intro_ending_video(self, manifest_scene: Dict[str, Any], paths: Dict[str, str], resolution: Tuple[int, int], script_type: str) -> bool:
        """ì¸íŠ¸ë¡œ/ì—”ë”© ë¹„ë””ì˜¤ ì œì‘ íŒŒì´í”„ë¼ì¸"""
        # ... (ì¸íŠ¸ë¡œ/ì—”ë”©ì— ë§ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤) ...
        return True # ì„±ê³µ ì—¬ë¶€ ë°˜í™˜