"""
íƒ€ì„ë¼ì¸ ìƒì„± ëª¨ë“ˆ

ë§¤ë‹ˆí˜ìŠ¤íŠ¸, ì˜¤ë””ì˜¤ íƒ€ì´ë° ì •ë³´, ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ë¥¼ ì¢…í•©í•˜ì—¬
ìµœì¢… ë¹„ë””ì˜¤ ë Œë”ë§ì— í•„ìš”í•œ íƒ€ì„ë¼ì¸ JSONì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import json
import glob
import wave
import struct
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

from ..core.context import PipelineContext


@dataclass
class TimelineEntry:
    """íƒ€ì„ë¼ì¸ ì—”íŠ¸ë¦¬"""
    scene_id: str
    start_time: float
    end_time: float
    duration: float
    image_path: str
    scene_type: str
    sequence: int


@dataclass
class TimelineData:
    """íƒ€ì„ë¼ì¸ ë°ì´í„°"""
    resolution: str
    final_audio_path: str
    timeline: List[TimelineEntry]
    total_duration: float


class TimelineGenerator:
    """íƒ€ì„ë¼ì¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.timeline_entries = []
    
    def _get_english_script_type(self, script_type: str) -> str:
        """ìŠ¤í¬ë¦½íŠ¸ íƒ€ì…ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜"""
        script_type_mapping = {
            "íšŒí™”": "conversation",
            "ëŒ€í™”": "dialogue",
            "ì¸íŠ¸ë¡œ": "intro",
            "ì—”ë”©": "ending"
        }
        return script_type_mapping.get(script_type, script_type.lower())
    
    def _get_audio_duration(self, audio_path: str) -> float:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ì‹¤ì œ ê¸¸ì´ë¥¼ ì¸¡ì • (ì´ˆ ë‹¨ìœ„)"""
        try:
            if not os.path.exists(audio_path):
                return 0.0
            
            import subprocess
            result = subprocess.run(
                ['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration', '-of', 'csv=p=0', audio_path],
                capture_output=True, text=True, check=True
            )
            duration = float(result.stdout.strip())
            return round(duration, 2)
            
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨: {e}")
            # ffprobe ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ì˜ íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¶”ì • ë¡œì§ì„ fallbackìœ¼ë¡œ ì‚¬ìš©
            try:
                if audio_path.lower().endswith('.mp3'):
                    file_size = os.path.getsize(audio_path)
                    bitrate = 128 * 1000
                    duration = (file_size * 8) / bitrate
                    return round(duration, 2)
            except Exception as fallback_e:
                print(f"âš ï¸ Fallback ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨: {fallback_e}")

            return 0.0
    
    def _create_default_timing(self, manifest_data: Dict) -> Dict:
        """ê¸°ë³¸ íƒ€ì´ë° ë°ì´í„° ìƒì„± (íƒ€ì´ë° íŒŒì¼ì´ ì—†ì„ ë•Œ ì‚¬ìš©)"""
        try:
            scenes = manifest_data.get('scenes', [])
            segments = []
            current_time = 0.0
            
            for scene in scenes:
                scene_type = scene.get('type', 'conversation')
                sequence = scene.get('sequence', 1)
                
                if scene_type == 'conversation':
                    # íšŒí™”ì˜ ê²½ìš° ì‹¤ì œ í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •
                    native_script = scene.get('native_script', '')
                    learning_script = scene.get('learning_script', '')
                    reading_script = scene.get('reading_script', '')
                    
                    # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ duration ì¶”ì • (í•œê¸€: ì•½ 3ì/ì´ˆ, ì˜ì–´: ì•½ 5ì/ì´ˆ)
                    screen1_duration = max(2.0, len(native_script) * 0.3)  # ìµœì†Œ 2ì´ˆ
                    screen2_duration = max(3.0, (len(learning_script) + len(reading_script)) * 0.3)  # ìµœì†Œ 3ì´ˆ
                    
                    segments.extend([
                        {
                            "name": f"scene_{sequence}_screen1_start_to_scene_{sequence}_screen1_end",
                            "start_time": current_time,
                            "end_time": current_time + screen1_duration,
                            "duration": screen1_duration
                        },
                        {
                            "name": f"scene_{sequence}_screen2_start_to_scene_{sequence}_screen2_end", 
                            "start_time": current_time + screen1_duration + 1.0,  # 1ì´ˆ ê°„ê²©
                            "end_time": current_time + screen1_duration + 1.0 + screen2_duration,
                            "duration": screen2_duration
                        }
                    ])
                    current_time += screen1_duration + 1.0 + screen2_duration + 1.0  # 1ì´ˆ ê°„ê²©
                else:
                    # ì¸íŠ¸ë¡œ/ì—”ë”©ì˜ ê²½ìš° í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜
                    full_script = scene.get('full_script', '')
                    duration = max(3.0, len(full_script) * 0.3)  # ìµœì†Œ 3ì´ˆ
                    
                    segments.append({
                        "name": f"scene_{sequence}_start_to_scene_{sequence}_end",
                        "start_time": current_time,
                        "end_time": current_time + duration,
                        "duration": duration
                    })
                    current_time += duration + 1.0  # 1ì´ˆ ê°„ê²©
            
            return {
                "segments": segments,
                "total_duration": current_time
            }
            
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ íƒ€ì´ë° ìƒì„± ì‹¤íŒ¨: {e}")
            return {"segments": [], "total_duration": 0.0}
    
    def _create_audio_based_timing(self, manifest_data: Dict, audio_path: str) -> Dict:
        """ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ íƒ€ì´ë° ë°ì´í„° ìƒì„±"""
        try:
            audio_duration = self._get_audio_duration(audio_path)
            if audio_duration <= 0:
                print("âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì¸¡ì •í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ íƒ€ì´ë°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._create_default_timing(manifest_data)
            
            scenes = manifest_data.get('scenes', [])
            segments = []
            current_time = 0.0
            
            # ì´ ì¥ë©´ ìˆ˜ ê³„ì‚°
            total_scenes = len(scenes)
            if total_scenes == 0:
                return {"segments": [], "total_duration": 0.0}
            
            # ê° ì¥ë©´ì— í• ë‹¹í•  ì‹œê°„ ê³„ì‚° (ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì¥ë©´ ìˆ˜ë¡œ ë‚˜ëˆ”)
            time_per_scene = audio_duration / total_scenes
            
            for scene in scenes:
                scene_type = scene.get('type', 'conversation')
                sequence = scene.get('sequence', 1)
                
                if scene_type == 'conversation':
                    # íšŒí™”ì˜ ê²½ìš° screen1(40%), screen2(60%) ë¹„ìœ¨ë¡œ ë¶„í• 
                    # ë¬´ìŒ ê¸°ê°„ì„ ê³ ë ¤í•˜ì—¬ ì‹¤ì œ ì˜¤ë””ì˜¤ ì‹œê°„ì—ì„œ 1ì´ˆë¥¼ ë¹¼ê³  ë¶„í• 
                    available_time = time_per_scene - 1.0  # 1ì´ˆ ë¬´ìŒ ê¸°ê°„ ì œì™¸
                    screen1_duration = available_time * 0.4
                    screen2_duration = available_time * 0.6
                    
                    segments.extend([
                        {
                            "name": f"scene_{sequence}_screen1_start_to_scene_{sequence}_screen1_end",
                            "start_time": current_time,
                            "end_time": current_time + screen1_duration,
                            "duration": screen1_duration
                        },
                        {
                            "name": f"scene_{sequence}_screen2_start_to_scene_{sequence}_screen2_end", 
                            "start_time": current_time + screen1_duration + 1.0,  # 1ì´ˆ ë¬´ìŒ ê¸°ê°„
                            "end_time": current_time + screen1_duration + 1.0 + screen2_duration,
                            "duration": screen2_duration
                        }
                    ])
                    current_time += time_per_scene
                else:
                    # ì¸íŠ¸ë¡œ/ì—”ë”©ì˜ ê²½ìš° ì „ì²´ ì‹œê°„ ì‚¬ìš©
                    segments.append({
                        "name": f"scene_{sequence}_start_to_scene_{sequence}_end",
                        "start_time": current_time,
                        "end_time": current_time + time_per_scene,
                        "duration": time_per_scene
                    })
                    current_time += time_per_scene
            
            print(f"ğŸµ ì˜¤ë””ì˜¤ ê¸°ë°˜ íƒ€ì´ë° ìƒì„±: ì´ {audio_duration:.2f}ì´ˆ, {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            return {
                "segments": segments,
                "total_duration": audio_duration
            }
            
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ê¸°ë°˜ íƒ€ì´ë° ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_default_timing(manifest_data)
    
    def _apply_conversation_timing_matching(self, raw_segments: List[Dict]) -> List[Dict]:
        """
        ì œì‘ ì‚¬ì–‘ì„œì— ë”°ë¥¸ íšŒí™” ë¹„ë””ì˜¤ íƒ€ì´ë° ë§¤ì¹­ ì ìš©
        
        í™”ì : ì›ì–´í™”ì, í•™ìŠµì–´ í™”ì 1,2,3,4 
        ëŒ€í™” ìˆœë²ˆ : ë‹¤ìŒê³¼ ê°™ì´ ì¬ìƒí•˜ë©° í™”ìê°„, í–‰ê°„ì—ëŠ” 1ì´ˆì˜ ë¬´ìŒì„ ë„£ì–´ì¤€ë‹¤.
            1. ì›ì–´í™”ì - ì›ì–´             í™”ë©´ 1   ì›ì–´ í™”ì ì‹œì‘ ì‹œê°„ ~ ì¢…ë£Œ ì‹œê°„
            2. í•™ìŠµì–´ í™”ì 1 - í•™ìŠµì–´       í™”ë©´ 2   í•™ìŠµì–´ í™”ì 1 ì‹œì‘ ì‹œê°„ ~ í•™ìŠµì–´ í™”ì 4 ì¢…ë£Œì‹œê°„
            3. í•™ìŠµì–´ í™”ì 2 - í•™ìŠµì–´       í™”ë©´ 2
            4. í•™ìŠµì–´ í™”ì 3 - í•™ìŠµì–´       í™”ë©´ 2
            5. í•™ìŠµì–´ í™”ì 4 - í•™ìŠµì–´       í™”ë©´ 2
        """
        try:
            print("ğŸ¬ ì œì‘ ì‚¬ì–‘ì„œì— ë”°ë¥¸ íšŒí™” íƒ€ì´ë° ë§¤ì¹­ ì ìš©...")
            
            # ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¥ë©´ë³„ë¡œ ê·¸ë£¹í™”
            scene_groups = {}
            for segment in raw_segments:
                scene_info = self._parse_segment_name(segment.get('name', ''))
                if scene_info:
                    sequence = scene_info['sequence']
                    screen_type = scene_info.get('screen_type', '')
                    
                    if sequence not in scene_groups:
                        scene_groups[sequence] = {}
                    scene_groups[sequence][screen_type] = segment
            
            # ê° ì¥ë©´ë³„ë¡œ íƒ€ì´ë° ë§¤ì¹­ ì ìš©
            matched_segments = []
            for sequence in sorted(scene_groups.keys()):
                scene_data = scene_groups[sequence]
                
                # í™”ë©´ 1: ì›ì–´í™”ì - ì›ì–´ (ì›ì–´ í™”ì ì‹œì‘ ì‹œê°„ ~ ì¢…ë£Œ ì‹œê°„)
                if 'screen1' in scene_data:
                    screen1_segment = scene_data['screen1'].copy()
                    screen1_segment['name'] = f"kor-chn_{sequence:03d}_screen1.png"
                    screen1_segment['scene_type'] = "conversation"
                    screen1_segment['sequence'] = sequence
                    screen1_segment['screen_type'] = "screen1"
                    matched_segments.append(screen1_segment)
                    print(f"   âœ… í™”ë©´ 1 (ì›ì–´): scene_{sequence} ({screen1_segment['start_time']:.2f}s ~ {screen1_segment['end_time']:.2f}s)")
                
                # í™”ë©´ 2: í•™ìŠµì–´ í™”ì 1,2,3,4 - í•™ìŠµì–´ (í•™ìŠµì–´ í™”ì 1 ì‹œì‘ ì‹œê°„ ~ í•™ìŠµì–´ í™”ì 4 ì¢…ë£Œ ì‹œê°„)
                if 'screen2' in scene_data:
                    screen2_segment = scene_data['screen2'].copy()
                    screen2_segment['name'] = f"kor-chn_{sequence:03d}_screen2.png"
                    screen2_segment['scene_type'] = "conversation"
                    screen2_segment['sequence'] = sequence
                    screen2_segment['screen_type'] = "screen2"
                    matched_segments.append(screen2_segment)
                    print(f"   âœ… í™”ë©´ 2 (í•™ìŠµì–´): scene_{sequence} ({screen2_segment['start_time']:.2f}s ~ {screen2_segment['end_time']:.2f}s)")
            
            print(f"ğŸ¬ íšŒí™” íƒ€ì´ë° ë§¤ì¹­ ì™„ë£Œ: {len(matched_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            return matched_segments
            
        except Exception as e:
            print(f"âŒ íšŒí™” íƒ€ì´ë° ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return raw_segments  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def _parse_segment_name(self, segment_name: str) -> Optional[Dict]:
        """ì„¸ê·¸ë¨¼íŠ¸ ì´ë¦„ì—ì„œ ì¥ë©´ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì˜ˆ: "scene_1_screen1_start_to_scene_1_screen1_end" -> {"sequence": 1, "screen_type": "screen1"}
            if "_start_to_" in segment_name:
                # ì‹œì‘ ë¶€ë¶„ë§Œ íŒŒì‹±
                start_part = segment_name.split("_start_to_")[0]
                parts = start_part.split("_")
                
                if len(parts) >= 3 and parts[0] == "scene":
                    sequence = int(parts[1])
                    screen_type = parts[2] if len(parts) > 2 else ""
                    
                    return {
                        "sequence": sequence,
                        "screen_type": screen_type,
                        "scene_type": "conversation" if screen_type else "intro"
                    }
            
            return None
            
        except (ValueError, IndexError) as e:
            print(f"âŒ ì„¸ê·¸ë¨¼íŠ¸ ì´ë¦„ íŒŒì‹± ì‹¤íŒ¨: {segment_name} - {e}")
            return None
    
    def _get_image_path_for_segment(self, context: PipelineContext, sequence: int, screen_type: str) -> Optional[str]:
        """ì„¸ê·¸ë¨¼íŠ¸ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        try:
            if screen_type:
                # íšŒí™”ì˜ ê²½ìš°: screen1, screen2
                if screen_type in ["screen1", "screen2"]:
                    # ìƒˆë¡œìš´ íŒŒì¼ëª… í˜•ì‹ ì‚¬ìš©: kor-chn_001_screen1.png
                    image_filename = f"{context.identifier}_{sequence:03d}_{screen_type}.png"
                    image_path = os.path.join(context.paths.conversation_dir, image_filename)
                    
                    # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    if os.path.exists(image_path):
                        return image_path
                    else:
                        print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {image_path}")
                        return None
            else:
                # ì¸íŠ¸ë¡œ/ì—”ë”©ì˜ ê²½ìš°
                image_filename = f"{context.identifier}_{sequence:03d}.png"
                image_path = os.path.join(context.paths.intro_ending_dir, image_filename)
                
                # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if os.path.exists(image_path):
                    return image_path
                else:
                    print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {image_path}")
                    return None
            
            return None
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ê²½ë¡œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def generate_timeline(self, context: PipelineContext) -> Optional[str]:
        """
        íƒ€ì„ë¼ì¸ ìƒì„±
        
        Args:
            context: íŒŒì´í”„ë¼ì¸ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            str: ìƒì„±ëœ íƒ€ì„ë¼ì¸ íŒŒì¼ ê²½ë¡œ
        """
        try:
            print(f"ğŸ¬ íƒ€ì„ë¼ì¸ ìƒì„± ì‹œì‘: {context.identifier}")
            
            # 1. ì…ë ¥ íŒŒì¼ ë¡œë“œ
            manifest_data = self._load_manifest(context)
            timing_data = self._load_timing(context)
            
            if not manifest_data:
                print("âŒ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # íƒ€ì´ë° íŒŒì¼ì´ ì—†ì–´ë„ ê¸°ë³¸ íƒ€ì„ë¼ì¸ ìƒì„± ê°€ëŠ¥
            if not timing_data:
                print("âš ï¸ íƒ€ì´ë° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‹¤ì œ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íƒ€ì´ë° ìƒì„±
                audio_path = self._get_final_audio_path(context)
                if audio_path and os.path.exists(audio_path):
                    print("ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ íƒ€ì´ë°ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                    timing_data = self._create_audio_based_timing(manifest_data, audio_path)
                else:
                    print("ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ íƒ€ì´ë°ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                    timing_data = self._create_default_timing(manifest_data)
            
            # 2. íƒ€ì„ë¼ì¸ ì´ˆê¸°í™”
            final_audio_path = self._get_final_audio_path(context)
            timeline_data = TimelineData(
                resolution=context.manifest.resolution or "1920x1080",
                final_audio_path=final_audio_path,
                timeline=[],
                total_duration=0.0
            )
            
            # ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ì—¬ total_duration ì„¤ì •
            if final_audio_path and os.path.exists(final_audio_path):
                try:
                    import subprocess
                    result = subprocess.run(['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration', '-of', 'csv=p=0', final_audio_path], capture_output=True, text=True)
                    actual_duration = float(result.stdout.strip())
                    timeline_data.total_duration = actual_duration
                    print(f"ğŸµ ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ ì‚¬ìš©: {actual_duration:.2f}ì´ˆ (ë¬´ìŒ í¬í•¨)")
                except Exception as e:
                    print(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨: {e}, íƒ€ì´ë° íŒŒì¼ì˜ total_duration ì‚¬ìš©")
                    # íƒ€ì´ë° íŒŒì¼ì˜ total_duration ì‚¬ìš©
                    if timing_data and 'total_duration' in timing_data:
                        timeline_data.total_duration = timing_data['total_duration']
                        print(f"ğŸµ íƒ€ì´ë° íŒŒì¼ì˜ total_duration ì‚¬ìš©: {timing_data['total_duration']:.2f}ì´ˆ")
            else:
                print(f"âš ï¸ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {final_audio_path}")
                # íƒ€ì´ë° íŒŒì¼ì˜ total_duration ì‚¬ìš©
                if timing_data and 'total_duration' in timing_data:
                    timeline_data.total_duration = timing_data['total_duration']
                    print(f"ğŸµ íƒ€ì´ë° íŒŒì¼ì˜ total_duration ì‚¬ìš©: {timing_data['total_duration']:.2f}ì´ˆ")
            
            # 3. íƒ€ì´ë° ì„¸ê·¸ë¨¼íŠ¸ ìˆœíšŒ ë° ë§¤ì¹­
            timing_segments = []
            
            # segments ë°°ì—´ì´ ì§ì ‘ ìˆëŠ” ê²½ìš° (ìš°ì„ ìˆœìœ„ 1)
            if 'segments' in timing_data:
                print("ğŸ“Š segments ë°°ì—´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
                raw_segments = timing_data.get('segments', [])
                
                # íšŒí™” ë¹„ë””ì˜¤ì˜ ê²½ìš° ì œì‘ ì‚¬ì–‘ì„œì— ë”°ë¥¸ íƒ€ì´ë° ë§¤ì¹­ ì ìš©
                if context.script_type in ["íšŒí™”", "ëŒ€í™”"]:
                    timing_segments = self._apply_conversation_timing_matching(raw_segments)
                else:
                    timing_segments = raw_segments
            
            # scenes ë°°ì—´ì´ ìˆìœ¼ë©´ segmentsë¡œ ë³€í™˜ (ìš°ì„ ìˆœìœ„ 2)
            elif 'scenes' in timing_data:
                print("ğŸ“Š scenes ë°°ì—´ì„ segmentsë¡œ ë³€í™˜í•©ë‹ˆë‹¤...")
                for scene in timing_data['scenes']:
                    sequence = scene.get('sequence', 1)
                    timings = scene.get('timings', {})
                    
                    # screen1 ì„¸ê·¸ë¨¼íŠ¸
                    if 'screen1' in timings:
                        screen1_timing = timings['screen1']
                        timing_segments.append({
                            "name": f"scene_{sequence}_screen1_start_to_scene_{sequence}_screen1_end",
                            "start_time": screen1_timing.get('start', 0) / 1000.0,  # msë¥¼ ì´ˆë¡œ ë³€í™˜
                            "end_time": screen1_timing.get('end', 0) / 1000.0,
                            "duration": (screen1_timing.get('end', 0) - screen1_timing.get('start', 0)) / 1000.0,
                            "scene_type": "conversation",
                            "sequence": sequence,
                            "screen_type": "screen1"
                        })
                    
                    # screen2 ì„¸ê·¸ë¨¼íŠ¸
                    if 'screen2' in timings:
                        screen2_timing = timings['screen2']
                        timing_segments.append({
                            "name": f"scene_{sequence}_screen2_start_to_scene_{sequence}_screen2_end",
                            "start_time": screen2_timing.get('start', 0) / 1000.0,  # msë¥¼ ì´ˆë¡œ ë³€í™˜
                            "end_time": screen2_timing.get('end', 0) / 1000.0,
                            "duration": (screen2_timing.get('end', 0) - screen2_timing.get('start', 0)) / 1000.0,
                            "scene_type": "conversation",
                            "sequence": sequence,
                            "screen_type": "screen2"
                        })
            
            # marks ë°°ì—´ì´ ìˆìœ¼ë©´ segmentsë¡œ ë³€í™˜ (ìš°ì„ ìˆœìœ„ 3 - ë§ˆì§€ë§‰)
            elif 'marks' in timing_data:
                print("ğŸ“Š marks ë°°ì—´ì„ segmentsë¡œ ë³€í™˜í•©ë‹ˆë‹¤...")
                marks = timing_data['marks']
                
                # marksë¥¼ ìŒìœ¼ë¡œ ê·¸ë£¹í™” (start, end)
                i = 0
                while i < len(marks) - 1:
                    start_mark = marks[i]
                    end_mark = marks[i + 1]
                    
                    if start_mark['name'].endswith('_start') and end_mark['name'].endswith('_end'):
                        # ì„¸ê·¸ë¨¼íŠ¸ ì´ë¦„ì—ì„œ ì¥ë©´ ì •ë³´ ì¶”ì¶œ
                        segment_name = f"{start_mark['name']}_to_{end_mark['name']}"
                        
                        # ì¥ë©´ ë²ˆí˜¸ì™€ ìŠ¤í¬ë¦° íƒ€ì… ì¶”ì¶œ
                        name_parts = start_mark['name'].split('_')
                        if len(name_parts) >= 3:
                            sequence = int(name_parts[1])
                            screen_type = name_parts[2]
                            
                            timing_segments.append({
                                "name": segment_name,
                                "start_time": start_mark['position'] / 1000.0,  # msë¥¼ ì´ˆë¡œ ë³€í™˜
                                "end_time": end_mark['position'] / 1000.0,
                                "duration": (end_mark['position'] - start_mark['position']) / 1000.0,
                                "scene_type": "conversation",
                                "sequence": sequence,
                                "screen_type": screen_type
                            })
                    
                    i += 2  # start, end ìŒìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ 2ì”© ì¦ê°€
            
            print(f"   - ì´ {len(timing_segments)}ê°œì˜ íƒ€ì´ë° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
            
            for segment in timing_segments:
                segment_name = segment.get('name', '')
                start_time = segment.get('start_time', 0.0)
                end_time = segment.get('end_time', 0.0)
                duration = segment.get('duration', 0.0)
                
                # 4. ì„¸ê·¸ë¨¼íŠ¸ ì´ë¦„ì—ì„œ ì¥ë©´ ì •ë³´ ì¶”ì¶œ
                scene_info = self._parse_segment_name(segment_name)
                if not scene_info:
                    print(f"âš ï¸ ì„¸ê·¸ë¨¼íŠ¸ ì´ë¦„ì„ íŒŒì‹±í•  ìˆ˜ ì—†ì–´ '{segment_name}'ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                scene_sequence = scene_info['sequence']
                screen_type = scene_info.get('screen_type', '')
                
                # 5. ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìƒì„± ë° í™•ì¸ (name í•„ë“œì—ì„œ ì§ì ‘ ì¶”ì¶œ)
                # name í•„ë“œê°€ ì´ë¯¸ ì™„ì „í•œ íŒŒì¼ëª…ì„ í¬í•¨í•˜ê³  ìˆìŒ (ì˜ˆ: kor-chn_001_screen1.png)
                image_filename = segment.get('name', '')
                if not image_filename:
                    print(f"âš ï¸ ì„¸ê·¸ë¨¼íŠ¸ì— name í•„ë“œê°€ ì—†ì–´ '{segment_name}'ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # íšŒí™”ì˜ ê²½ìš° conversation ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸°
                if screen_type in ["screen1", "screen2"]:
                    image_path = os.path.join(context.paths.conversation_dir, image_filename)
                else:
                    # ì¸íŠ¸ë¡œ/ì—”ë”©ì˜ ê²½ìš°
                    image_path = os.path.join(context.paths.intro_ending_dir, image_filename)
                
                if not os.path.exists(image_path):
                    print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ì–´ '{segment_name}'ì„ ê±´ë„ˆëœë‹ˆë‹¤. ê²½ë¡œ: {image_path}")
                    continue
                
                # 6. íƒ€ì„ë¼ì¸ ì—”íŠ¸ë¦¬ ìƒì„±
                entry = TimelineEntry(
                    scene_id=segment_name,
                    start_time=start_time,
                    end_time=end_time,
                    duration=duration,
                    image_path=image_path,
                    scene_type=scene_info.get('scene_type', 'conversation'),
                    sequence=scene_sequence
                )
                
                timeline_data.timeline.append(entry)
                # total_durationì€ ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
                print(f"   -> íƒ€ì„ë¼ì¸ ì¶”ê°€: {segment_name} ({start_time:.2f}s ~ {end_time:.2f}s)")
            
            # 7. íƒ€ì„ë¼ì¸ íŒŒì¼ ì €ì¥
            # total_durationì´ 0ì´ë©´ ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ë¥¼ ë‹¤ì‹œ ì¸¡ì •
            if timeline_data.total_duration <= 0:
                if final_audio_path and os.path.exists(final_audio_path):
                    try:
                        import subprocess
                        result = subprocess.run(['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration', '-of', 'csv=p=0', final_audio_path], capture_output=True, text=True)
                        actual_duration = float(result.stdout.strip())
                        timeline_data.total_duration = actual_duration
                        print(f"ğŸµ ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ ì‚¬ìš©: {actual_duration:.2f}ì´ˆ")
                    except Exception as e:
                        print(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨: {e}")
                        # ì„¸ê·¸ë¨¼íŠ¸ ìµœëŒ€ end_time ì‚¬ìš©
                        max_end_time = 0.0
                        for entry in timeline_data.timeline:
                            max_end_time = max(max_end_time, entry.end_time)
                        timeline_data.total_duration = max_end_time
                        print(f"âš ï¸ ì„¸ê·¸ë¨¼íŠ¸ ìµœëŒ€ end_time ì‚¬ìš©: {max_end_time:.1f}ì´ˆ")
                else:
                    # ì„¸ê·¸ë¨¼íŠ¸ ìµœëŒ€ end_time ì‚¬ìš©
                    max_end_time = 0.0
                    for entry in timeline_data.timeline:
                        max_end_time = max(max_end_time, entry.end_time)
                    timeline_data.total_duration = max_end_time
                    print(f"âš ï¸ ì„¸ê·¸ë¨¼íŠ¸ ìµœëŒ€ end_time ì‚¬ìš©: {max_end_time:.1f}ì´ˆ")
            
            timeline_path = self._save_timeline(context, timeline_data)
            
            if timeline_path:
                print(f"âœ… íƒ€ì„ë¼ì¸ ìƒì„± ì™„ë£Œ: {timeline_path}")
                print(f"   - ì´ {len(timeline_data.timeline)}ê°œ ì—”íŠ¸ë¦¬")
                print(f"   - ì´ ì¬ìƒì‹œê°„: {timeline_data.total_duration:.1f}ì´ˆ")
            
            return timeline_path
            
        except Exception as e:
            print(f"âŒ íƒ€ì„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _load_manifest(self, context: PipelineContext) -> Optional[Dict]:
        """ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ"""
        try:
            english_script_type = self._get_english_script_type(context.script_type)
            
            manifest_path = os.path.join(
                context.paths.manifest_dir,
                f"{context.identifier}_{english_script_type}.json"
            )
            
            if not os.path.exists(manifest_path):
                print(f"âŒ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {manifest_path}")
                return None
            
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest_data = json.load(f)
            
            print(f"âœ… ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {manifest_path}")
            return manifest_data
            
        except Exception as e:
            print(f"âŒ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_timing(self, context: PipelineContext) -> Optional[Dict]:
        """ì •í™•í•œ íƒ€ì´ë° íŒŒì¼ ë¡œë“œ (ì„¸ê·¸ë¨¼íŠ¸ë³„ ì˜¤ë””ì˜¤ ê¸°ë°˜)"""
        try:
            english_script_type = self._get_english_script_type(context.script_type)
            
            # ì •í™•í•œ íƒ€ì´ë° íŒŒì¼ ê²½ë¡œ (ì„¸ê·¸ë¨¼íŠ¸ë³„ ì˜¤ë””ì˜¤ ê¸°ë°˜)
            precise_timing_path = os.path.join(
                context.paths.timing_dir,
                f"{context.identifier}_{english_script_type}.json"
            )
            
            # ê¸°ì¡´ ì •í™•í•œ íƒ€ì´ë° íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©
            if os.path.exists(precise_timing_path):
                with open(precise_timing_path, 'r', encoding='utf-8') as f:
                    timing_data = json.load(f)
                    print(f"âœ… ì •í™•í•œ íƒ€ì´ë° íŒŒì¼ ë¡œë“œ: {precise_timing_path}")
                    print(f"   - ì´ {len(timing_data.get('segments', []))}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
                    print(f"   - ì´ ì¬ìƒì‹œê°„: {timing_data.get('total_duration', 0):.2f}ì´ˆ")
                    return timing_data
            
            # ì •í™•í•œ íƒ€ì´ë° íŒŒì¼ì´ ì—†ìœ¼ë©´ None ë°˜í™˜ (ê¸°ë³¸ íƒ€ì´ë° ìƒì„±)
            print(f"âš ï¸ ì •í™•í•œ íƒ€ì´ë° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {precise_timing_path}")
            return None
            
        except Exception as e:
            print(f"âŒ íƒ€ì´ë° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    
    
    def _get_final_audio_path(self, context: PipelineContext) -> str:
        """ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        try:
            english_script_type = self._get_english_script_type(context.script_type)
            
            # 1. mp3 ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            mp3_path = os.path.join(
                context.paths.output_dir,
                "mp3",
                f"{context.identifier}_{english_script_type}.mp3"
            )
            if os.path.exists(mp3_path):
                return mp3_path
            
            # 2. audio ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            audio_path = os.path.join(
                context.paths.audio_dir,
                f"{context.identifier}_{english_script_type}.mp3"
            )
            if os.path.exists(audio_path):
                return audio_path
            
            # 3. fallback: audio.mp3
            fallback_path = os.path.join(context.paths.audio_dir, "audio.mp3")
            if os.path.exists(fallback_path):
                return fallback_path
            
            return ""
            
        except Exception:
            return ""
    
    def _save_timeline(self, context: PipelineContext, timeline_data: TimelineData) -> Optional[str]:
        """íƒ€ì„ë¼ì¸ íŒŒì¼ ì €ì¥"""
        try:
            # íƒ€ì„ë¼ì¸ ë””ë ‰í† ë¦¬ ìƒì„±
            timeline_dir = os.path.join(context.paths.output_dir, "timeline")
            os.makedirs(timeline_dir, exist_ok=True)
            
            english_script_type = self._get_english_script_type(context.script_type)
            
            # íŒŒì¼ëª… ìƒì„±
            timeline_filename = f"{context.identifier}_{english_script_type}.json"
            timeline_path = os.path.join(timeline_dir, timeline_filename)
            
            # íƒ€ì„ë¼ì¸ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
            timeline_json = {
                "resolution": timeline_data.resolution,
                "final_audio_path": timeline_data.final_audio_path,
                "total_duration": timeline_data.total_duration,
                "timeline": [
                    {
                        "scene_id": entry.scene_id,
                        "start_time": entry.start_time,
                        "end_time": entry.end_time,
                        "duration": entry.duration,
                        "image_path": entry.image_path,
                        "scene_type": entry.scene_type,
                        "sequence": entry.sequence
                    }
                    for entry in timeline_data.timeline
                ]
            }
            
            # íŒŒì¼ ì €ì¥
            with open(timeline_path, 'w', encoding='utf-8') as f:
                json.dump(timeline_json, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“ íƒ€ì„ë¼ì¸ ì €ì¥: {timeline_path}")
            return timeline_path
            
        except Exception as e:
            print(f"âŒ íƒ€ì„ë¼ì¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None


def run(context: PipelineContext) -> Optional[str]:
    """
    íƒ€ì„ë¼ì¸ ìƒì„± ì‹¤í–‰
    
    Args:
        context: íŒŒì´í”„ë¼ì¸ ì»¨í…ìŠ¤íŠ¸
        
    Returns:
        str: ìƒì„±ëœ íƒ€ì„ë¼ì¸ íŒŒì¼ ê²½ë¡œ
    """
    generator = TimelineGenerator()
    return generator.generate_timeline(context)
