"""
í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ë° ë³´ê³ ì„œ ìƒì„± ì‹œìŠ¤í…œ

ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ í†µí•©í•˜ì—¬ ì‹¤í–‰í•˜ê³  ì¢…í•©ì ì¸ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œìž…ë‹ˆë‹¤.
"""

import os
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
from .setting_reflection_test import SettingReflectionTest
from .system_integration_test import SystemIntegrationTest
from .performance_test import PerformanceTest


class TestRunner:
    """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, output_dir: str = "test_results"):
        self.output_dir = output_dir
        self.test_suites: List[Dict[str, Any]] = []
        self.overall_results: Dict[str, Any] = {}
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì´ˆê¸°í™”
        self._initialize_test_suites()
    
    def _initialize_test_suites(self) -> None:
        """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì´ˆê¸°í™”"""
        self.test_suites = [
            {
                'name': 'setting_reflection_test',
                'class': SettingReflectionTest,
                'description': 'ì„¤ì • ë°˜ì˜ í…ŒìŠ¤íŠ¸',
                'enabled': True
            },
            {
                'name': 'system_integration_test',
                'class': SystemIntegrationTest,
                'description': 'ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸',
                'enabled': True
            },
            {
                'name': 'performance_test',
                'class': PerformanceTest,
                'description': 'ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬',
                'enabled': True
            }
        ]
    
    def run_all_tests(self, include_performance: bool = True) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("=" * 80)
        print("ðŸš€ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì‹œìž‘")
        print("=" * 80)
        
        start_time = datetime.now()
        overall_results = {
            'start_time': start_time.isoformat(),
            'test_suites': {},
            'summary': {
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 0,
                'success_rate': 0.0
            }
        }
        
        # ê° í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰
        for suite in self.test_suites:
            if not suite['enabled']:
                print(f"â­ï¸  {suite['description']} ê±´ë„ˆë›°ê¸° (ë¹„í™œì„±í™”)")
                continue
            
            # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ëŠ” ì„ íƒì ìœ¼ë¡œ ì‹¤í–‰
            if suite['name'] == 'performance_test' and not include_performance:
                print(f"â­ï¸  {suite['description']} ê±´ë„ˆë›°ê¸° (ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì œì™¸)")
                continue
            
            print(f"\nðŸ“‹ {suite['description']} ì‹¤í–‰ ì¤‘...")
            print("-" * 60)
            
            try:
                suite_results = self._run_test_suite(suite)
                overall_results['test_suites'][suite['name']] = suite_results
                
                # ìš”ì•½ í†µê³„ ì—…ë°ì´íŠ¸
                if 'summary' in suite_results:
                    summary = suite_results['summary']
                    overall_results['summary']['total_tests'] += summary.get('total_tests', 0)
                    overall_results['summary']['passed_tests'] += summary.get('passed_tests', 0)
                    overall_results['summary']['failed_tests'] += summary.get('failed_tests', 0)
                
                print(f"âœ… {suite['description']} ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ {suite['description']} ì‹¤íŒ¨: {e}")
                overall_results['test_suites'][suite['name']] = {
                    'error': str(e),
                    'status': 'failed'
                }
        
        # ì „ì²´ ì„±ê³µë¥  ê³„ì‚°
        if overall_results['summary']['total_tests'] > 0:
            overall_results['summary']['success_rate'] = (
                overall_results['summary']['passed_tests'] / 
                overall_results['summary']['total_tests'] * 100
            )
        
        end_time = datetime.now()
        overall_results['end_time'] = end_time.isoformat()
        overall_results['duration'] = (end_time - start_time).total_seconds()
        
        self.overall_results = overall_results
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_summary()
        
        # ë³´ê³ ì„œ ìƒì„±
        self._generate_comprehensive_report()
        
        return overall_results
    
    def _run_test_suite(self, suite: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œë³„ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        suite_name = suite['name']
        suite_class = suite['class']
        
        # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        log_file = os.path.join(self.output_dir, f"{suite_name}.log")
        test_instance = suite_class(log_file)
        
        suite_results = {
            'name': suite_name,
            'description': suite['description'],
            'start_time': datetime.now().isoformat(),
            'status': 'running'
        }
        
        try:
            if suite_name == 'setting_reflection_test':
                # ì„¤ì • ë°˜ì˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                test_instance.add_default_test_cases()
                test_results = test_instance.run_all_tests()
                suite_results['test_results'] = test_results
                suite_results['summary'] = test_instance.get_test_summary()
                suite_results['report'] = test_instance.generate_test_report()
                
            elif suite_name == 'system_integration_test':
                # ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                test_results = test_instance.run_all_integration_tests()
                suite_results['test_results'] = test_results
                suite_results['summary'] = test_instance.get_integration_summary()
                suite_results['report'] = test_instance.generate_integration_report()
                
            elif suite_name == 'performance_test':
                # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                performance_results = {}
                
                # ë Œë”ë§ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
                print("  ðŸ”„ ë Œë”ë§ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰...")
                rendering_results = test_instance.run_rendering_performance_benchmark(iterations=5)
                performance_results['rendering_benchmark'] = rendering_results
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
                print("  ðŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
                memory_results = test_instance.run_memory_usage_test(iterations=3)
                performance_results['memory_usage'] = memory_results
                
                # ë™ì‹œ ë Œë”ë§ í…ŒìŠ¤íŠ¸
                print("  âš¡ ë™ì‹œ ë Œë”ë§ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
                concurrent_results = test_instance.run_concurrent_rendering_test(concurrent_count=2)
                performance_results['concurrent_rendering'] = concurrent_results
                
                suite_results['performance_results'] = performance_results
                suite_results['report'] = test_instance.generate_performance_report()
                
                # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìš”ì•½
                suite_results['summary'] = {
                    'total_tests': 3,  # ë Œë”ë§, ë©”ëª¨ë¦¬, ë™ì‹œ ë Œë”ë§
                    'passed_tests': sum(1 for result in performance_results.values() if result),
                    'failed_tests': sum(1 for result in performance_results.values() if not result),
                    'success_rate': (sum(1 for result in performance_results.values() if result) / 3 * 100) if performance_results else 0
                }
            
            suite_results['end_time'] = datetime.now().isoformat()
            suite_results['status'] = 'completed'
            
            # ê°œë³„ ë³´ê³ ì„œ ì €ìž¥
            report_file = os.path.join(self.output_dir, f"{suite_name}_report.txt")
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(suite_results['report'])
            
            # ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
            results_file = os.path.join(self.output_dir, f"{suite_name}_results.json")
            if suite_name == 'setting_reflection_test':
                test_instance.export_test_results(results_file)
            elif suite_name == 'system_integration_test':
                test_instance.export_integration_results(results_file)
            elif suite_name == 'performance_test':
                test_instance.export_performance_results(results_file)
            
            return suite_results
            
        except Exception as e:
            suite_results['error'] = str(e)
            suite_results['status'] = 'failed'
            suite_results['end_time'] = datetime.now().isoformat()
            return suite_results
    
    def _print_summary(self) -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ðŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        
        summary = self.overall_results['summary']
        print(f"ì´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}ê°œ")
        print(f"í†µê³¼: {summary['passed_tests']}ê°œ")
        print(f"ì‹¤íŒ¨: {summary['failed_tests']}ê°œ")
        print(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        print(f"ì´ ì‹¤í–‰ ì‹œê°„: {self.overall_results['duration']:.2f}ì´ˆ")
        
        print("\nðŸ“‹ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ë³„ ê²°ê³¼:")
        for suite_name, suite_results in self.overall_results['test_suites'].items():
            if 'error' in suite_results:
                print(f"  âŒ {suite_name}: ì˜¤ë¥˜ - {suite_results['error']}")
            else:
                suite_summary = suite_results.get('summary', {})
                success_rate = suite_summary.get('success_rate', 0)
                status_icon = "âœ…" if success_rate >= 80 else "âš ï¸" if success_rate >= 60 else "âŒ"
                print(f"  {status_icon} {suite_name}: {success_rate:.1f}% ì„±ê³µ")
    
    def _generate_comprehensive_report(self) -> None:
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        report_file = os.path.join(self.output_dir, "comprehensive_test_report.txt")
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("ðŸŽ¯ í†µí•© í…ŒìŠ¤íŠ¸ ì¢…í•© ë³´ê³ ì„œ\n")
                f.write("=" * 80 + "\n")
                f.write(f"ìƒì„± ì‹œê°„: {datetime.now()}\n")
                f.write(f"ì‹¤í–‰ ì‹œê°„: {self.overall_results['start_time']} ~ {self.overall_results['end_time']}\n")
                f.write(f"ì´ ì‹¤í–‰ ì‹œê°„: {self.overall_results['duration']:.2f}ì´ˆ\n\n")
                
                # ì „ì²´ ìš”ì•½
                summary = self.overall_results['summary']
                f.write("ðŸ“Š ì „ì²´ ìš”ì•½\n")
                f.write("-" * 40 + "\n")
                f.write(f"ì´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}ê°œ\n")
                f.write(f"í†µê³¼: {summary['passed_tests']}ê°œ\n")
                f.write(f"ì‹¤íŒ¨: {summary['failed_tests']}ê°œ\n")
                f.write(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}%\n\n")
                
                # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ë³„ ìƒì„¸ ê²°ê³¼
                f.write("ðŸ“‹ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ë³„ ìƒì„¸ ê²°ê³¼\n")
                f.write("-" * 40 + "\n")
                
                for suite_name, suite_results in self.overall_results['test_suites'].items():
                    f.write(f"\nðŸ” {suite_results.get('description', suite_name)}\n")
                    f.write(f"ìƒíƒœ: {suite_results.get('status', 'unknown')}\n")
                    
                    if 'error' in suite_results:
                        f.write(f"ì˜¤ë¥˜: {suite_results['error']}\n")
                    else:
                        suite_summary = suite_results.get('summary', {})
                        f.write(f"ì´ í…ŒìŠ¤íŠ¸: {suite_summary.get('total_tests', 0)}ê°œ\n")
                        f.write(f"í†µê³¼: {suite_summary.get('passed_tests', 0)}ê°œ\n")
                        f.write(f"ì‹¤íŒ¨: {suite_summary.get('failed_tests', 0)}ê°œ\n")
                        f.write(f"ì„±ê³µë¥ : {suite_summary.get('success_rate', 0):.1f}%\n")
                        
                        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ íŠ¹ë³„ ì²˜ë¦¬
                        if suite_name == 'performance_test' and 'performance_results' in suite_results:
                            perf_results = suite_results['performance_results']
                            f.write("\nì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n")
                            
                            if 'rendering_benchmark' in perf_results and perf_results['rendering_benchmark']:
                                rendering = perf_results['rendering_benchmark']
                                f.write(f"  - ë Œë”ë§ ë²¤ì¹˜ë§ˆí¬: {rendering.get('total_time', 0):.3f}ì´ˆ\n")
                            
                            if 'memory_usage' in perf_results and perf_results['memory_usage']:
                                memory = perf_results['memory_usage']
                                f.write(f"  - ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory.get('peak_memory', 0):.2f}MB\n")
                            
                            if 'concurrent_rendering' in perf_results and perf_results['concurrent_rendering']:
                                concurrent = perf_results['concurrent_rendering']
                                f.write(f"  - ë™ì‹œ ë Œë”ë§: {concurrent.get('success_count', 0)}/{concurrent.get('concurrent_count', 0)} ì„±ê³µ\n")
                
                f.write("\n" + "=" * 80 + "\n")
                f.write("ðŸ“ ìƒì„¸ ë³´ê³ ì„œ íŒŒì¼ë“¤:\n")
                f.write("-" * 40 + "\n")
                f.write("- setting_reflection_test_report.txt: ì„¤ì • ë°˜ì˜ í…ŒìŠ¤íŠ¸ ìƒì„¸ ë³´ê³ ì„œ\n")
                f.write("- system_integration_test_report.txt: ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ìƒì„¸ ë³´ê³ ì„œ\n")
                f.write("- performance_test_report.txt: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìƒì„¸ ë³´ê³ ì„œ\n")
                f.write("- *_results.json: ê° í…ŒìŠ¤íŠ¸ì˜ JSON ê²°ê³¼ íŒŒì¼ë“¤\n")
                f.write("- *.log: ê° í…ŒìŠ¤íŠ¸ì˜ ë¡œê·¸ íŒŒì¼ë“¤\n")
            
            print(f"\nðŸ“„ ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_file}")
            
        except Exception as e:
            print(f"âŒ ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def export_overall_results(self, filepath: str = None) -> bool:
        """ì „ì²´ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        if filepath is None:
            filepath = os.path.join(self.output_dir, "overall_test_results.json")
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.overall_results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"ðŸ“Š ì „ì²´ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filepath}")
            return True
            
        except Exception as e:
            print(f"âŒ ì „ì²´ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def get_test_summary(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return {
            'overall_summary': self.overall_results.get('summary', {}),
            'test_suites_count': len(self.overall_results.get('test_suites', {})),
            'total_duration': self.overall_results.get('duration', 0),
            'output_directory': self.output_dir
        }
